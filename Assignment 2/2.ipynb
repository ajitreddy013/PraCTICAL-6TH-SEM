{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importing libraries \n",
        "# Importing essential libraries for data analysis and visualization\n",
        "import pandas as pd  # For data manipulation and analysis\n",
        "import numpy as np   # For numerical operations\n",
        "import matplotlib.pyplot as plt  # For creating visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read the csv file into a pandas dataframe\n",
        "# Read the academic data CSV file into a pandas DataFrame\n",
        "# This will load the student records containing their names, gender and subject marks\n",
        "df = pd.read_csv(\"acdemic_data.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display the first 5 rows of the DataFrame using head() method\n",
        "# This gives us a quick overview of the data structure and content\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1 --Define a list of common missing value formats that may appear in the dataset\n",
        "# These formats include various ways missing values might be represented in the data\n",
        "missing_value_formats = [\n",
        "    \"n.a.\",  # Common abbreviation for \"not available\"\n",
        "    \"?\",     # Question mark often used to indicate unknown values\n",
        "    \"NA\",    # Standard \"not available\" abbreviation\n",
        "    \"n/a\",   # Another common \"not available\" format\n",
        "    \"na\",    # Lowercase version of NA\n",
        "    \"--\"     # Double dash sometimes used to represent missing values\n",
        "]\n",
        "# Read the CSV file with custom NA value formats\n",
        "# This ensures consistent handling of missing values across different formats\n",
        "# The na_values parameter specifies additional strings to recognize as NA/NaN\n",
        "df = pd.read_csv(\"acdemic_data.csv\", na_values=missing_value_formats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display the first 10 rows of the Gender column from the DataFrame\n",
        "# This helps verify the data after handling missing values and shows the gender distribution\n",
        "print(df['Gender'].head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1 -- Null values are marked True\n",
        "# Check for null values in the Gender column and display first 10 results\n",
        "# Returns True for null values and False for non-null values\n",
        "print(df['Gender'].isnull().head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# notnull() returns False for NaN values and True for non-NaN values\n",
        "# This is the opposite of isnull() - it marks all valid values as True\n",
        "# and all missing/NaN values as False\n",
        "print(df['Gender'].notnull().head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# notnull() is a pandas method that returns a boolean mask where:\n",
        "# - True indicates non-null values (valid data)\n",
        "# - False indicates null values (NaN, None, etc.)\n",
        "# This is useful for filtering out rows with missing data\n",
        "null_filter = df['Gender'].notnull()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter and display only rows where Gender column has valid (non-null) values\n",
        "# This uses the boolean mask created earlier to show only complete records\n",
        "# The null_filter variable contains True for valid entries and False for null values\n",
        "print(df[null_filter]) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if there are any null values in the entire DataFrame\n",
        "# df.isnull() creates a boolean mask of all null values\n",
        "# .values converts the mask to a numpy array\n",
        "# .any() returns True if any value in the array is True (indicating presence of null values)\n",
        "print(df.isnull().values.any())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Drop all rows that contain any null values (NaN, None, etc.)\n",
        "# axis=0 specifies we're dropping rows (axis=1 would drop columns)\n",
        "# inplace=True modifies the DataFrame directly instead of returning a copy\n",
        "# This is a common data cleaning step to remove incomplete records\n",
        "df.dropna(axis=0, inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Drop all rows that contain at least one null value (NaN, None, etc.)\n",
        "# This is a data cleaning operation that removes incomplete records\n",
        "# - axis=0 specifies we're dropping rows (not columns)\n",
        "# - how='any' means drop if ANY column in the row has a null value\n",
        "# - Returns a new DataFrame with only complete rows\n",
        "new_df = df.dropna(axis = 0, how ='any')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# drop all rows with all null\n",
        "new_df = df.dropna(axis = 0, how ='all')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Drop all columns that contain at least one null value (NaN, None, etc.)\n",
        "# - axis=1 specifies we're dropping columns (not rows)\n",
        "# - how='any' means drop if ANY row in the column has a null value\n",
        "# - Returns a new DataFrame with only columns that have no null values\n",
        "# This is useful for removing columns with incomplete data\n",
        "new_df = df.dropna(axis=1, how='any')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Drop all columns that contain only null values (NaN, None, etc.)\n",
        "# - axis=1 specifies we're dropping columns (not rows)\n",
        "# - how='all' means drop if ALL values in the column are null\n",
        "# - Returns a new DataFrame with only columns that have at least one non-null value\n",
        "# This is useful for removing completely empty columns from the dataset\n",
        "new_df = df.dropna(axis = 1, how ='all')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Replacing Null values with a constant value (0 in this case)\n",
        "# - fillna() is used to replace null values with a specified value\n",
        "# - inplace=True modifies the DataFrame directly instead of returning a copy\n",
        "# - This is useful when you want to replace missing values with a meaningful default value\n",
        "# - Common use case: replacing missing numeric values with 0 or another meaningful constant\n",
        "df['SPOS'].fillna(0, inplace=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# To check changes call \n",
        "# - head() method displays the first n rows of the DataFrame\n",
        "# - n=10 specifies we want to see the first 10 rows\n",
        "# - This helps verify that our null value replacements worked correctly\n",
        "# - We can see the SPOS column values after filling nulls with 0\n",
        "print(df['SPOS'].head(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Replacing Null with the value from the previous row or the next row \n",
        "#method = 'padâ€™ for taking values from the previous row \n",
        "# Replacing Null values with values from adjacent rows\n",
        "# - method='pad' uses forward fill (ffill) to take values from previous rows\n",
        "# - inplace=True modifies the DataFrame directly instead of returning a copy\n",
        "# - This is useful when you want to fill missing values with the most recent valid value\n",
        "# - Common use case: time series data where missing values can be reasonably filled with previous values\n",
        "\n",
        "df['DSBDA'] = df['DSBDA'].ffill()\n",
        "print(df['DSBDA'].head(10))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Using backward fill (bfill) to replace null values with values from subsequent rows\n",
        "# - method='bfill' uses backward fill to take values from next rows\n",
        "# - This is useful when forward fill isn't appropriate and you want to use future values\n",
        "# - Common use case: when missing values should be filled with the next available value\n",
        "df['SPOS'] = df['SPOS'].bfill()\n",
        "print(df['SPOS'].head(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display the first 5 rows of the DataFrame by default\n",
        "# - head() method shows the first n rows (default n=5)\n",
        "# - Useful for quick inspection of data structure and content\n",
        "# - Shows column names and data types\n",
        "# - Helps verify data cleaning operations\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the dimensions of the DataFrame\n",
        "# - Returns a tuple containing (rows, columns)\n",
        "# - First number represents total number of rows\n",
        "# - Second number represents total number of columns\n",
        "# - Useful for understanding the size and structure of the dataset\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a box plot to visualize the distribution of SPOS scores\n",
        "# - x parameter specifies the data to plot (SPOS column from DataFrame)\n",
        "# - Box plot shows:\n",
        "#   * Median (middle line)\n",
        "#   * First and third quartiles (box)\n",
        "#   * Whiskers (extend to min/max excluding outliers)\n",
        "#   * Individual points represent outliers\n",
        "# - Useful for identifying:\n",
        "#   * Data distribution\n",
        "#   * Potential outliers\n",
        "#   * Data spread and skewness\n",
        "plt.boxplot(x=df['SPOS'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3---Define numeric columns for normalization\n",
        "# In this section, we performed data normalization using MinMax scaling on our academic dataset.\n",
        "# The process involved:\n",
        "# 1. Identifying numeric columns (SPOS, DSBDA, WT, DA) containing continuous academic scores\n",
        "# 2. Using MinMaxScaler to transform these values to a range between 0 and 1\n",
        "# 3. Creating new columns with '_minmax' suffix to store the normalized values\n",
        "# 4. Preserving the original data while adding the transformed values\n",
        "# This normalization helps in:\n",
        "# - Comparing different subjects' scores on the same scale\n",
        "# - Making the data more suitable for machine learning algorithms\n",
        "# - Reducing the impact of different score ranges across subjects\n",
        "# SPOS: System Programming and Operating System scores\n",
        "# DSBDA: Data Science and Big Data Analytics scores\n",
        "# WT: Web Technology scores\n",
        "# DA: Data Analytics scores\n",
        "# We'll use these columns for MinMax scaling to bring all values between 0 and 1\n",
        "numeric_columns = ['SPOS', 'DSBDA', 'WT', 'DA']  # These are your numeric columns from the academic data\n",
        "\n",
        "# Initialize MinMaxScaler to normalize data between 0 and 1\n",
        "# - MinMaxScaler transforms features by scaling each feature to a given range\n",
        "# - Default range is [0,1] which is suitable for our academic scores\n",
        "# - This helps in comparing different subjects' scores on the same scale\n",
        "# - Preserves the shape of the original distribution\n",
        "# - Formula: X_std = (X - X.min) / (X.max - X.min)\n",
        "# - X_scaled = X_std * (max - min) + min\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "df_transformed = df.copy()  # Create a copy of original dataframe\n",
        "scaled_data = scaler.fit_transform(df[numeric_columns])\n",
        "\n",
        "# Create new column names for transformed data\n",
        "transformed_columns = [col + '_minmax' for col in numeric_columns]\n",
        "\n",
        "# Add transformed data to dataframe\n",
        "df_transformed[transformed_columns] = scaled_data\n",
        "\n",
        "# Print to verify the transformation\n",
        "print(df_transformed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3--Data Type Analysis and Log Transformation\n",
        "# In this section, we performed two important data preprocessing steps:\n",
        "# 1. Data Type Verification:\n",
        "#    - We first checked the data types of our numeric columns (SPOS, DSBDA, WT, DA)\n",
        "#    - This helps ensure our data is in the correct format for further processing\n",
        "# 2. Log Transformation:\n",
        "#    - We converted all columns to numeric type using pd.to_numeric()\n",
        "#    - Applied log1p transformation (log(1+x)) to handle:\n",
        "#      * Zero values in the data\n",
        "#      * Reduce skewness in the distribution\n",
        "#      * Make the data more normally distributed\n",
        "#      * Handle outliers by compressing the scale\n",
        "#    - Created new columns with '_log' suffix for transformed values\n",
        "# This process helps in:\n",
        "# - Normalizing the data distribution\n",
        "# - Making the data more suitable for statistical analysis\n",
        "# - Reducing the impact of extreme values\n",
        "# - Improving the performance of certain machine learning algorithms\n",
        "print(\"Data types of columns:\")\n",
        "print(df[numeric_columns].dtypes)\n",
        "\n",
        "# Convert columns to numeric type and handle log transformation\n",
        "for col in numeric_columns:\n",
        "    # Convert to numeric, coerce errors to NaN\n",
        "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "    \n",
        "    # Apply log transformation only to positive values\n",
        "    # Using log1p (log(1+x)) to handle zero values\n",
        "    df_transformed[f'{col}_log'] = np.log1p(df[col])\n",
        "\n",
        "# Print to verify transformation\n",
        "print(\"\\nTransformed Data:\")\n",
        "print(df_transformed.head())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPCabucTToaeyp9H9XKn7xe",
      "include_colab_link": true,
      "name": "A2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
