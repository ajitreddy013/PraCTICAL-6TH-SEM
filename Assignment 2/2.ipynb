{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importing libraries \n",
        "# Importing essential libraries for data analysis and visualization\n",
        "import pandas as pd  # For data manipulation and analysis\n",
        "import numpy as np   # For numerical operations\n",
        "import matplotlib.pyplot as plt  # For creating visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read the csv file into a pandas dataframe\n",
        "# Read the academic data CSV file into a pandas DataFrame\n",
        "# This will load the student records containing their names, gender and subject marks\n",
        "df = pd.read_csv(\"acdemic_data.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Firstname Lastname Gender  SPOS DSBDA     WT  DA\n",
            "0        LA      Roy      M  85.0    88   90.0  92\n",
            "1        SA    Dixit      F  90.0    90   94.0  95\n",
            "2        AB   Danial      M   NaN    na  100.0  80\n",
            "3        DA   Kapoor      M  95.0    86  500.0  82\n",
            "4        SA      Jha      F   NaN    84   98.0  84\n"
          ]
        }
      ],
      "source": [
        "# Display the first 5 rows of the DataFrame using head() method\n",
        "# This gives us a quick overview of the data structure and content\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1 --Define a list of common missing value formats that may appear in the dataset\n",
        "# These formats include various ways missing values might be represented in the data\n",
        "missing_value_formats = [\n",
        "    \"n.a.\",  # Common abbreviation for \"not available\"\n",
        "    \"?\",     # Question mark often used to indicate unknown values\n",
        "    \"NA\",    # Standard \"not available\" abbreviation\n",
        "    \"n/a\",   # Another common \"not available\" format\n",
        "    \"na\",    # Lowercase version of NA\n",
        "    \"--\"     # Double dash sometimes used to represent missing values\n",
        "]\n",
        "# Read the CSV file with custom NA value formats\n",
        "# This ensures consistent handling of missing values across different formats\n",
        "# The na_values parameter specifies additional strings to recognize as NA/NaN\n",
        "df = pd.read_csv(\"acdemic_data.csv\", na_values=missing_value_formats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0    M\n",
            "1    F\n",
            "2    M\n",
            "3    M\n",
            "4    F\n",
            "5    F\n",
            "6    M\n",
            "7    F\n",
            "8    M\n",
            "9    M\n",
            "Name: Gender, dtype: object\n"
          ]
        }
      ],
      "source": [
        "# Display the first 10 rows of the Gender column from the DataFrame\n",
        "# This helps verify the data after handling missing values and shows the gender distribution\n",
        "print(df['Gender'].head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0    False\n",
            "1    False\n",
            "2    False\n",
            "3    False\n",
            "4    False\n",
            "5    False\n",
            "6    False\n",
            "7    False\n",
            "8    False\n",
            "9    False\n",
            "Name: Gender, dtype: bool\n"
          ]
        }
      ],
      "source": [
        "# 1 -- Null values are marked True\n",
        "# Check for null values in the Gender column and display first 10 results\n",
        "# Returns True for null values and False for non-null values\n",
        "print(df['Gender'].isnull().head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0    True\n",
            "1    True\n",
            "2    True\n",
            "3    True\n",
            "4    True\n",
            "5    True\n",
            "6    True\n",
            "7    True\n",
            "8    True\n",
            "9    True\n",
            "Name: Gender, dtype: bool\n"
          ]
        }
      ],
      "source": [
        "# notnull() returns False for NaN values and True for non-NaN values\n",
        "# This is the opposite of isnull() - it marks all valid values as True\n",
        "# and all missing/NaN values as False\n",
        "print(df['Gender'].notnull().head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# notnull() is a pandas method that returns a boolean mask where:\n",
        "# - True indicates non-null values (valid data)\n",
        "# - False indicates null values (NaN, None, etc.)\n",
        "# This is useful for filtering out rows with missing data\n",
        "null_filter = df['Gender'].notnull()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Firstname Lastname Gender   SPOS  DSBDA     WT   DA\n",
            "0         LA      Roy      M   85.0   88.0   90.0   92\n",
            "1         SA    Dixit      F   90.0   90.0   94.0   95\n",
            "2         AB   Danial      M    NaN    NaN  100.0   80\n",
            "3         DA   Kapoor      M   95.0   86.0  500.0   82\n",
            "4         SA      Jha      F    NaN   84.0   98.0   84\n",
            "5         WS   Thakur      F   87.0    NaN   97.0   86\n",
            "6         DY   Kapoor      M   81.0   80.0   96.0   89\n",
            "7         JK   Khanna      F   40.0   88.0   95.0   87\n",
            "8         RT  Pardesi      M   10.0   96.0   94.0   88\n",
            "9         UV  Pardesi      M    9.0    NaN   93.0   96\n",
            "10        MN    Desai      F  200.0   87.0    NaN  N.A\n",
            "11        BC    Patel      F  300.0    NaN   88.0  300\n"
          ]
        }
      ],
      "source": [
        "# Filter and display only rows where Gender column has valid (non-null) values\n",
        "# This uses the boolean mask created earlier to show only complete records\n",
        "# The null_filter variable contains True for valid entries and False for null values\n",
        "print(df[null_filter]) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "# Check if there are any null values in the entire DataFrame\n",
        "# df.isnull() creates a boolean mask of all null values\n",
        "# .values converts the mask to a numpy array\n",
        "# .any() returns True if any value in the array is True (indicating presence of null values)\n",
        "print(df.isnull().values.any())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Drop all rows that contain any null values (NaN, None, etc.)\n",
        "# axis=0 specifies we're dropping rows (axis=1 would drop columns)\n",
        "# inplace=True modifies the DataFrame directly instead of returning a copy\n",
        "# This is a common data cleaning step to remove incomplete records\n",
        "df.dropna(axis=0, inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Drop all rows that contain at least one null value (NaN, None, etc.)\n",
        "# This is a data cleaning operation that removes incomplete records\n",
        "# - axis=0 specifies we're dropping rows (not columns)\n",
        "# - how='any' means drop if ANY column in the row has a null value\n",
        "# - Returns a new DataFrame with only complete rows\n",
        "new_df = df.dropna(axis = 0, how ='any')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# drop all rows with all null\n",
        "new_df = df.dropna(axis = 0, how ='all')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Drop all columns that contain at least one null value (NaN, None, etc.)\n",
        "# - axis=1 specifies we're dropping columns (not rows)\n",
        "# - how='any' means drop if ANY row in the column has a null value\n",
        "# - Returns a new DataFrame with only columns that have no null values\n",
        "# This is useful for removing columns with incomplete data\n",
        "new_df = df.dropna(axis=1, how='any')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Drop all columns that contain only null values (NaN, None, etc.)\n",
        "# - axis=1 specifies we're dropping columns (not rows)\n",
        "# - how='all' means drop if ALL values in the column are null\n",
        "# - Returns a new DataFrame with only columns that have at least one non-null value\n",
        "# This is useful for removing completely empty columns from the dataset\n",
        "new_df = df.dropna(axis = 1, how ='all')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/2j/73zzfhw16y5c_5v2q9j0mxvr0000gn/T/ipykernel_18743/2424766147.py:6: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['SPOS'].fillna(0, inplace=True)\n"
          ]
        }
      ],
      "source": [
        "# Replacing Null values with a constant value (0 in this case)\n",
        "# - fillna() is used to replace null values with a specified value\n",
        "# - inplace=True modifies the DataFrame directly instead of returning a copy\n",
        "# - This is useful when you want to replace missing values with a meaningful default value\n",
        "# - Common use case: replacing missing numeric values with 0 or another meaningful constant\n",
        "df['SPOS'].fillna(0, inplace=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0    85.0\n",
            "1    90.0\n",
            "3    95.0\n",
            "6    81.0\n",
            "7    40.0\n",
            "8    10.0\n",
            "Name: SPOS, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# To check changes call \n",
        "# - head() method displays the first n rows of the DataFrame\n",
        "# - n=10 specifies we want to see the first 10 rows\n",
        "# - This helps verify that our null value replacements worked correctly\n",
        "# - We can see the SPOS column values after filling nulls with 0\n",
        "print(df['SPOS'].head(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0    88.0\n",
            "1    90.0\n",
            "3    86.0\n",
            "6    80.0\n",
            "7    88.0\n",
            "8    96.0\n",
            "Name: DSBDA, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "#Replacing Null with the value from the previous row or the next row \n",
        "#method = 'pad’ for taking values from the previous row \n",
        "# Replacing Null values with values from adjacent rows\n",
        "# - method='pad' uses forward fill (ffill) to take values from previous rows\n",
        "# - inplace=True modifies the DataFrame directly instead of returning a copy\n",
        "# - This is useful when you want to fill missing values with the most recent valid value\n",
        "# - Common use case: time series data where missing values can be reasonably filled with previous values\n",
        "\n",
        "df['DSBDA'] = df['DSBDA'].ffill()\n",
        "print(df['DSBDA'].head(10))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0    85.0\n",
            "1    90.0\n",
            "3    95.0\n",
            "6    81.0\n",
            "7    40.0\n",
            "8    10.0\n",
            "Name: SPOS, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# Using backward fill (bfill) to replace null values with values from subsequent rows\n",
        "# - method='bfill' uses backward fill to take values from next rows\n",
        "# - This is useful when forward fill isn't appropriate and you want to use future values\n",
        "# - Common use case: when missing values should be filled with the next available value\n",
        "df['SPOS'] = df['SPOS'].bfill()\n",
        "print(df['SPOS'].head(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Firstname</th>\n",
              "      <th>Lastname</th>\n",
              "      <th>Gender</th>\n",
              "      <th>SPOS</th>\n",
              "      <th>DSBDA</th>\n",
              "      <th>WT</th>\n",
              "      <th>DA</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LA</td>\n",
              "      <td>Roy</td>\n",
              "      <td>M</td>\n",
              "      <td>85.0</td>\n",
              "      <td>88.0</td>\n",
              "      <td>90.0</td>\n",
              "      <td>92</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>SA</td>\n",
              "      <td>Dixit</td>\n",
              "      <td>F</td>\n",
              "      <td>90.0</td>\n",
              "      <td>90.0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>95</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>DA</td>\n",
              "      <td>Kapoor</td>\n",
              "      <td>M</td>\n",
              "      <td>95.0</td>\n",
              "      <td>86.0</td>\n",
              "      <td>500.0</td>\n",
              "      <td>82</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>DY</td>\n",
              "      <td>Kapoor</td>\n",
              "      <td>M</td>\n",
              "      <td>81.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>96.0</td>\n",
              "      <td>89</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>JK</td>\n",
              "      <td>Khanna</td>\n",
              "      <td>F</td>\n",
              "      <td>40.0</td>\n",
              "      <td>88.0</td>\n",
              "      <td>95.0</td>\n",
              "      <td>87</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Firstname Lastname Gender  SPOS  DSBDA     WT  DA\n",
              "0        LA      Roy      M  85.0   88.0   90.0  92\n",
              "1        SA    Dixit      F  90.0   90.0   94.0  95\n",
              "3        DA   Kapoor      M  95.0   86.0  500.0  82\n",
              "6        DY   Kapoor      M  81.0   80.0   96.0  89\n",
              "7        JK   Khanna      F  40.0   88.0   95.0  87"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Display the first 5 rows of the DataFrame by default\n",
        "# - head() method shows the first n rows (default n=5)\n",
        "# - Useful for quick inspection of data structure and content\n",
        "# - Shows column names and data types\n",
        "# - Helps verify data cleaning operations\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(6, 7)"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get the dimensions of the DataFrame\n",
        "# - Returns a tuple containing (rows, columns)\n",
        "# - First number represents total number of rows\n",
        "# - Second number represents total number of columns\n",
        "# - Useful for understanding the size and structure of the dataset\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'whiskers': [<matplotlib.lines.Line2D at 0x14b3ba780>,\n",
              "  <matplotlib.lines.Line2D at 0x14b3baae0>],\n",
              " 'caps': [<matplotlib.lines.Line2D at 0x14b3badb0>,\n",
              "  <matplotlib.lines.Line2D at 0x14b3bb0b0>],\n",
              " 'boxes': [<matplotlib.lines.Line2D at 0x14b3ba5d0>],\n",
              " 'medians': [<matplotlib.lines.Line2D at 0x14b3bb3b0>],\n",
              " 'fliers': [<matplotlib.lines.Line2D at 0x14b3bb680>],\n",
              " 'means': []}"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Create a box plot to visualize the distribution of SPOS scores\n",
        "# - x parameter specifies the data to plot (SPOS column from DataFrame)\n",
        "# - Box plot shows:\n",
        "#   * Median (middle line)\n",
        "#   * First and third quartiles (box)\n",
        "#   * Whiskers (extend to min/max excluding outliers)\n",
        "#   * Individual points represent outliers\n",
        "# - Useful for identifying:\n",
        "#   * Data distribution\n",
        "#   * Potential outliers\n",
        "#   * Data spread and skewness\n",
        "plt.boxplot(x=df['SPOS'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Firstname Lastname Gender  SPOS  DSBDA     WT  DA  SPOS_minmax  \\\n",
            "0        LA      Roy      M  85.0   88.0   90.0  92     0.882353   \n",
            "1        SA    Dixit      F  90.0   90.0   94.0  95     0.941176   \n",
            "3        DA   Kapoor      M  95.0   86.0  500.0  82     1.000000   \n",
            "6        DY   Kapoor      M  81.0   80.0   96.0  89     0.835294   \n",
            "7        JK   Khanna      F  40.0   88.0   95.0  87     0.352941   \n",
            "8        RT  Pardesi      M  10.0   96.0   94.0  88     0.000000   \n",
            "\n",
            "   DSBDA_minmax  WT_minmax  DA_minmax  \n",
            "0         0.500   0.000000   0.769231  \n",
            "1         0.625   0.009756   1.000000  \n",
            "3         0.375   1.000000   0.000000  \n",
            "6         0.000   0.014634   0.538462  \n",
            "7         0.500   0.012195   0.384615  \n",
            "8         1.000   0.009756   0.461538  \n"
          ]
        }
      ],
      "source": [
        "# 3---Define numeric columns for normalization\n",
        "# In this section, we performed data normalization using MinMax scaling on our academic dataset.\n",
        "# The process involved:\n",
        "# 1. Identifying numeric columns (SPOS, DSBDA, WT, DA) containing continuous academic scores\n",
        "# 2. Using MinMaxScaler to transform these values to a range between 0 and 1\n",
        "# 3. Creating new columns with '_minmax' suffix to store the normalized values\n",
        "# 4. Preserving the original data while adding the transformed values\n",
        "# This normalization helps in:\n",
        "# - Comparing different subjects' scores on the same scale\n",
        "# - Making the data more suitable for machine learning algorithms\n",
        "# - Reducing the impact of different score ranges across subjects\n",
        "# SPOS: System Programming and Operating System scores\n",
        "# DSBDA: Data Science and Big Data Analytics scores\n",
        "# WT: Web Technology scores\n",
        "# DA: Data Analytics scores\n",
        "# We'll use these columns for MinMax scaling to bring all values between 0 and 1\n",
        "numeric_columns = ['SPOS', 'DSBDA', 'WT', 'DA']  # These are your numeric columns from the academic data\n",
        "\n",
        "# Initialize MinMaxScaler to normalize data between 0 and 1\n",
        "# - MinMaxScaler transforms features by scaling each feature to a given range\n",
        "# - Default range is [0,1] which is suitable for our academic scores\n",
        "# - This helps in comparing different subjects' scores on the same scale\n",
        "# - Preserves the shape of the original distribution\n",
        "# - Formula: X_std = (X - X.min) / (X.max - X.min)\n",
        "# - X_scaled = X_std * (max - min) + min\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "df_transformed = df.copy()  # Create a copy of original dataframe\n",
        "scaled_data = scaler.fit_transform(df[numeric_columns])\n",
        "\n",
        "# Create new column names for transformed data\n",
        "transformed_columns = [col + '_minmax' for col in numeric_columns]\n",
        "\n",
        "# Add transformed data to dataframe\n",
        "df_transformed[transformed_columns] = scaled_data\n",
        "\n",
        "# Print to verify the transformation\n",
        "print(df_transformed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data types of columns:\n",
            "SPOS     float64\n",
            "DSBDA    float64\n",
            "WT       float64\n",
            "DA        object\n",
            "dtype: object\n",
            "\n",
            "Transformed Data:\n",
            "  Firstname Lastname Gender  SPOS  DSBDA     WT  DA  SPOS_minmax  \\\n",
            "0        LA      Roy      M  85.0   88.0   90.0  92     0.882353   \n",
            "1        SA    Dixit      F  90.0   90.0   94.0  95     0.941176   \n",
            "3        DA   Kapoor      M  95.0   86.0  500.0  82     1.000000   \n",
            "6        DY   Kapoor      M  81.0   80.0   96.0  89     0.835294   \n",
            "7        JK   Khanna      F  40.0   88.0   95.0  87     0.352941   \n",
            "\n",
            "   DSBDA_minmax  WT_minmax  DA_minmax  SPOS_log  DSBDA_log    WT_log    DA_log  \n",
            "0         0.500   0.000000   0.769231  4.454347   4.488636  4.510860  4.532599  \n",
            "1         0.625   0.009756   1.000000  4.510860   4.510860  4.553877  4.564348  \n",
            "3         0.375   1.000000   0.000000  4.564348   4.465908  6.216606  4.418841  \n",
            "6         0.000   0.014634   0.538462  4.406719   4.394449  4.574711  4.499810  \n",
            "7         0.500   0.012195   0.384615  3.713572   4.488636  4.564348  4.477337  \n"
          ]
        }
      ],
      "source": [
        "# 3--Data Type Analysis and Log Transformation\n",
        "# In this section, we performed two important data preprocessing steps:\n",
        "# 1. Data Type Verification:\n",
        "#    - We first checked the data types of our numeric columns (SPOS, DSBDA, WT, DA)\n",
        "#    - This helps ensure our data is in the correct format for further processing\n",
        "# 2. Log Transformation:\n",
        "#    - We converted all columns to numeric type using pd.to_numeric()\n",
        "#    - Applied log1p transformation (log(1+x)) to handle:\n",
        "#      * Zero values in the data\n",
        "#      * Reduce skewness in the distribution\n",
        "#      * Make the data more normally distributed\n",
        "#      * Handle outliers by compressing the scale\n",
        "#    - Created new columns with '_log' suffix for transformed values\n",
        "# This process helps in:\n",
        "# - Normalizing the data distribution\n",
        "# - Making the data more suitable for statistical analysis\n",
        "# - Reducing the impact of extreme values\n",
        "# - Improving the performance of certain machine learning algorithms\n",
        "print(\"Data types of columns:\")\n",
        "print(df[numeric_columns].dtypes)\n",
        "\n",
        "# Convert columns to numeric type and handle log transformation\n",
        "for col in numeric_columns:\n",
        "    # Convert to numeric, coerce errors to NaN\n",
        "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "    \n",
        "    # Apply log transformation only to positive values\n",
        "    # Using log1p (log(1+x)) to handle zero values\n",
        "    df_transformed[f'{col}_log'] = np.log1p(df[col])\n",
        "\n",
        "# Print to verify transformation\n",
        "print(\"\\nTransformed Data:\")\n",
        "print(df_transformed.head())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPCabucTToaeyp9H9XKn7xe",
      "include_colab_link": true,
      "name": "A2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
