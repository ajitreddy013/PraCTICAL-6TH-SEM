{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6431b8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Natural Language Toolkit (NLTK) library\n",
    "# NLTK is a powerful Python library for natural language processing tasks\n",
    "import nltk\n",
    "nltk.download('punkt', download_dir='./nltk_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e4dde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the opening line from Jane Austen's Pride and Prejudice\n",
    "# The text will be used for natural language processing examples\n",
    "text = \"It is a truth universally acknowledged, that a single man in possession of a good fortune,  must be in want of a wife.\" \n",
    "text = text.lower() \n",
    "print(text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ef668f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the opening line from Jane Austen's Pride and Prejudice (1813)\n",
    "# The text will be used as a sample for natural language processing tasks\n",
    "# The sentence is a famous example of irony and social commentary in literature\n",
    "text = \"It is a truth universally acknowledged, that a single man in possession of a good fortune,  must be in want of a wife.\" \n",
    "text = text.lower() \n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172250e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the string module which provides constants for ASCII characters\n",
    "# This module is particularly useful for handling punctuation and other special characters\n",
    "import string \n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc9c78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all punctuation marks from the text using string.punctuation\n",
    "# This creates a clean version of the text without any special characters\n",
    "text_p = \"\".join([char for char in text if char not in string.punctuation]); print(text_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9807e063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Natural Language Toolkit (NLTK) library\n",
    "# NLTK is a comprehensive Python library for natural language processing\n",
    "# It provides tools for tokenization, stemming, tagging, parsing, and more\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "071a6b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import word_tokenize and sent_tokenize from NLTK's tokenize module\n",
    "# word_tokenize splits text into individual words\n",
    "# sent_tokenize splits text into sentences\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bf42f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1-- Tokenize the cleaned text (text_p) into individual words using NLTK's word_tokenize\n",
    "# This splits the text into a list of words, handling contractions and special cases\n",
    "words = word_tokenize(text_p)\n",
    "words1 = sent_tokenize(text_p)\n",
    "print(words)\n",
    "print(words1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132a2774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1--Import NLTK library for natural language processing tasks\n",
    "import nltk\n",
    "\n",
    "# Import word_tokenize and sent_tokenize for splitting text into words and sentences\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# Import stopwords to remove common words that don't add meaning (e.g., 'the', 'is', 'at')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Import string module for handling punctuation and special characters\n",
    "import string\n",
    "\n",
    "# Download necessary NLTK data files\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Sample text\n",
    "text = \"It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife.\"\n",
    "\n",
    "# Remove punctuation\n",
    "text_p = \"\".join([char for char in text if char not in string.punctuation])\n",
    "\n",
    "# Tokenize\n",
    "words = word_tokenize(text_p)\n",
    "sentences = sent_tokenize(text_p)\n",
    "\n",
    "# Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "\n",
    "# Print results\n",
    "print(\"Words:\", words)\n",
    "print(\"Filtered Words:\", filtered_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49835763",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1-- Filter out stopwords from the tokenized words list\n",
    "# This creates a new list containing only meaningful words by removing common words like 'the', 'is', 'at'\n",
    "# The list comprehension checks each word against the stop_words set and only keeps non-stopwords\n",
    "filtered_words = [word for word in words if word not in stop_words] \n",
    "print(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c2ae25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1--- Import PorterStemmer from NLTK's stem package\n",
    "# PorterStemmer is a stemming algorithm that reduces words to their root form\n",
    "# For example: \"running\" -> \"run\", \"jumps\" -> \"jump\"\n",
    "from nltk.stem.porter import PorterStemmer \n",
    "porter = PorterStemmer() \n",
    "stemmed = [porter.stem(word) for word in filtered_words] \n",
    "print(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eedffcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Natural Language Toolkit (NLTK) library\n",
    "# NLTK is a leading platform for building Python programs to work with human language data\n",
    "# It provides easy-to-use interfaces to over 50 corpora and lexical resources\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52acc7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1--Import pos_tag from NLTK's tag package\n",
    "# pos_tag is a part-of-speech tagger that assigns grammatical categories to words\n",
    "# For example: \"run\" -> \"VB\" (verb), \"cat\" -> \"NN\" (noun)\n",
    "from nltk import pos_tag\n",
    "pos = pos_tag(filtered_words) \n",
    "print(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d9775f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import WordNetLemmatizer for lemmatization\n",
    "# Lemmatization is the process of reducing words to their base or dictionary form (lemma)\n",
    "# Unlike stemming which just chops off word endings, lemmatization considers the context and converts words to their meaningful base form\n",
    "# For example: \"better\" -> \"good\", \"running\" -> \"run\", \"mice\" -> \"mouse\"\n",
    "# WordNetLemmatizer uses WordNet's built-in morphy function to find the lemma of a word\n",
    "# This is more accurate than stemming but requires more computational resources\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
    "print(\"Lemmatized Words:\", lemmatized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6a5d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TfidfVectorizer from scikit-learn's text feature extraction module\n",
    "# This class implements the TF-IDF (Term Frequency-Inverse Document Frequency) algorithm\n",
    "# TF-IDF is a numerical statistic that reflects how important a word is to a document in a collection\n",
    "# It combines two metrics:\n",
    "# 1. Term Frequency (TF): How often a word appears in a document\n",
    "# 2. Inverse Document Frequency (IDF): How important the word is across all documents\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Import TfidfVectorizer again (this line appears to be redundant and can be removed)\n",
    "# The same class is already imported above\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create a corpus by joining the filtered words into a single string\n",
    "# The corpus is a collection of documents, here we're using a single document\n",
    "# \" \".join() concatenates all words with spaces in between\n",
    "corpus = [\" \".join(filtered_words)]\n",
    "\n",
    "# Initialize the TfidfVectorizer object\n",
    "# This will be used to convert the text data into TF-IDF features\n",
    "# By default, it uses:\n",
    "# - lowercase=True: converts all characters to lowercase\n",
    "# - stop_words='english': removes common English words\n",
    "# - token_pattern=r'(?u)\\b\\w\\w+\\b': pattern to extract words\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit the vectorizer to the corpus and transform the text into TF-IDF features\n",
    "# fit_transform() performs two operations:\n",
    "# 1. fit(): learns the vocabulary and IDF weights from the corpus\n",
    "# 2. transform(): converts the text into TF-IDF features\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Convert the sparse matrix to a dense array and print it\n",
    "# toarray() converts the sparse matrix representation to a regular numpy array\n",
    "# This makes it easier to visualize the TF-IDF values\n",
    "print(\"TF-IDF Representation:\")\n",
    "print(tfidf_matrix.toarray())\n",
    "\n",
    "# Print the feature names (words) that were extracted from the corpus\n",
    "# These are the words that were used to create the TF-IDF features\n",
    "# get_feature_names_out() returns an array of the feature names\n",
    "print(\"Feature Names (Words):\")\n",
    "print(vectorizer.get_feature_names_out())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6043dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is TF-IDF?\n",
    "# TF-IDF = Term Frequency × Inverse Document Frequency\n",
    "\n",
    "# TF (Term Frequency):\n",
    "# How frequently a word appears in a document.\n",
    "# Example: In the sentence \"Data is important in data science\", TF for \"data\" = 2/6\n",
    "\n",
    "# IDF (Inverse Document Frequency):\n",
    "# Measures how unique a word is across all documents.\n",
    "# Rare words get higher scores than common words like \"the\", \"is\", etc.\n",
    "\n",
    "# TF-IDF highlights important and unique words in a document."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
