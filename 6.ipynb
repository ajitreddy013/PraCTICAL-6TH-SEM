{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c8abf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np  # For numerical operations\n",
    "import matplotlib.pyplot as plt  # For plotting\n",
    "import pandas as pd  # For data manipulation and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a0295d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris dataset from GitHub repository\n",
    "# This dataset contains measurements of iris flowers including sepal length, sepal width, petal length, and petal width\n",
    "# The target variable is the species of iris (setosa, versicolor, or virginica)\n",
    "dataset = pd.read_csv('https://raw.githubusercontent.com/mk-gurucharan/Classification/master/IrisDataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66396ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Data Exploration and Analysis\n",
    "# This step involves examining the dataset's statistical properties and dimensions\n",
    "# - describe(): Generates descriptive statistics (count, mean, std, min, 25%, 50%, 75%, max)\n",
    "# - shape: Shows the number of rows and columns in the dataset\n",
    "# - head(): Displays the first few rows to understand the data structure\n",
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e789bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 5 rows of the dataset to understand its structure\n",
    "# This shows the features (sepal length, sepal width, petal length, petal width) and target variable (species)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde349f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dimensions of the dataset\n",
    "# Returns a tuple containing (number of rows, number of columns)\n",
    "# This helps understand the size and structure of our dataset\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a770d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features (X) from the dataset\n",
    "# Using iloc to select all rows (:) and first 4 columns (:4)\n",
    "# This selects the numerical features: sepal length, sepal width, petal length, and petal width\n",
    "# .values converts the pandas DataFrame to a numpy array for machine learning\n",
    "X = dataset.iloc[:,:4].values\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd531bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract target variable (y) from the dataset\n",
    "# Using the 'species' column which contains the iris species labels\n",
    "# .values converts the pandas Series to a numpy array for machine learning\n",
    "y = dataset['species'].values\n",
    "y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06fc9aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train_test_split from scikit-learn's model_selection module\n",
    "# This function splits arrays or matrices into random train and test subsets\n",
    "# - Useful for evaluating model performance on unseen data\n",
    "# - Returns X_train, X_test, y_train, y_test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f12c4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import StandardScaler from scikit-learn's preprocessing module\n",
    "# StandardScaler standardizes features by removing the mean and scaling to unit variance\n",
    "# - Centers the data by removing the mean of each feature\n",
    "# - Scales the data to unit variance (standard deviation = 1)\n",
    "# - This helps improve model performance by normalizing the feature scales\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c41225f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the standardized test features (X_test)\n",
    "# This shows the scaled test data after applying StandardScaler\n",
    "# Each row represents a test sample with 4 standardized features\n",
    "# Values are centered around 0 with unit variance\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8bc603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import GaussianNB from scikit-learn's naive_bayes module\n",
    "# GaussianNB implements the Gaussian Naive Bayes algorithm for classification\n",
    "# - Assumes features follow a normal distribution\n",
    "# - Simple and efficient for small to medium-sized datasets\n",
    "# - Works well with continuous features\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c8b641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the trained classifier to make predictions on the test data\n",
    "# - classifier.predict() returns predicted class labels for X_test\n",
    "# - Each prediction corresponds to a sample in X_test\n",
    "# - Returns numpy array of predicted class labels\n",
    "y_pred = classifier.predict(X_test) \n",
    "y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b30d9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import confusion_matrix from scikit-learn's metrics module\n",
    "# confusion_matrix computes the confusion matrix to evaluate classification accuracy\n",
    "# - Shows true positives, false positives, true negatives, and false negatives\n",
    "# - Helps visualize model performance and identify misclassifications\n",
    "# - Returns a 2D array where rows represent actual classes and columns represent predicted classes\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6616abdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import accuracy_score from scikit-learn's metrics module\n",
    "# accuracy_score calculates the accuracy of the classifier's predictions\n",
    "# - Compares predicted labels (y_pred) with true labels (y_test)\n",
    "# - Returns a float between 0 and 1 representing the proportion of correct predictions\n",
    "# - 1.0 means perfect accuracy, 0.0 means all predictions were wrong\n",
    "from sklearn.metrics import accuracy_score \n",
    "print (\"Accuracy : \", accuracy_score(y_test, y_pred))\n",
    "Accuracy :  1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61e1d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame to compare actual vs predicted values\n",
    "# - Uses pandas DataFrame to organize the results\n",
    "# - 'Real Values' column contains the true labels from y_test\n",
    "# - 'Predicted Values' column contains the model's predictions from y_pred\n",
    "# - Makes it easy to visually inspect model performance\n",
    "# - Helps identify where predictions match or differ from actual values\n",
    "df = pd.DataFrame({'Real Values':y_test, 'Predicted Values':y_pred})\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 13.707806,
   "end_time": "2022-03-28T06:30:19.758101",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-03-28T06:30:06.050295",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
