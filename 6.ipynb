{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c8abf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np  # For numerical operations\n",
    "import matplotlib.pyplot as plt  # For plotting\n",
    "import pandas as pd  # For data manipulation and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a0295d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris dataset from GitHub repository\n",
    "# This dataset contains measurements of iris flowers including sepal length, sepal width, petal length, and petal width\n",
    "# The target variable is the species of iris (setosa, versicolor, or virginica)\n",
    "dataset = pd.read_csv('https://raw.githubusercontent.com/mk-gurucharan/Classification/master/IrisDataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66396ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Data Exploration and Analysis\n",
    "# This step involves examining the dataset's statistical properties and dimensions\n",
    "# - describe(): Generates descriptive statistics (count, mean, std, min, 25%, 50%, 75%, max)\n",
    "# - shape: Shows the number of rows and columns in the dataset\n",
    "# - head(): Displays the first few rows to understand the data structure\n",
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e789bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 5 rows of the dataset to understand its structure\n",
    "# This shows the features (sepal length, sepal width, petal length, petal width) and target variable (species)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde349f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dimensions of the dataset\n",
    "# Returns a tuple containing (number of rows, number of columns)\n",
    "# This helps understand the size and structure of our dataset\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a770d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features (X) from the dataset\n",
    "# Using iloc to select all rows (:) and first 4 columns (:4)\n",
    "# This selects the numerical features: sepal length, sepal width, petal length, and petal width\n",
    "# .values converts the pandas DataFrame to a numpy array for machine learning\n",
    "X = dataset.iloc[:,:4].values\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd531bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract target variable (y) from the dataset\n",
    "# Using the 'species' column which contains the iris species labels\n",
    "# .values converts the pandas Series to a numpy array for machine learning\n",
    "y = dataset['species'].values\n",
    "y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06fc9aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train_test_split from scikit-learn's model_selection module\n",
    "# This function splits arrays or matrices into random train and test subsets\n",
    "# - Useful for evaluating model performance on unseen data\n",
    "# - Returns X_train, X_test, y_train, y_test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f12c4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import StandardScaler from scikit-learn's preprocessing module\n",
    "# StandardScaler standardizes features by removing the mean and scaling to unit variance\n",
    "# - Centers the data by removing the mean of each feature\n",
    "# - Scales the data to unit variance (standard deviation = 1)\n",
    "# - This helps improve model performance by normalizing the feature scales\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c41225f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the standardized test features (X_test)\n",
    "# This shows the scaled test data after applying StandardScaler\n",
    "# Each row represents a test sample with 4 standardized features\n",
    "# Values are centered around 0 with unit variance\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8bc603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1--Simple Naïve Bayes Classification\n",
    "# Naïve Bayes is a probabilistic classifier based on Bayes’ Theorem\n",
    "# with a strong (naïve) assumption that features are independent \n",
    "# given the class.Import GaussianNB from scikit-learn's \n",
    "# naive_bayes module\n",
    "# Bayes' Theorem describes the probability of an event based on prior \n",
    "# knowledge of conditions related to the event.\n",
    "# GaussianNB implements the Gaussian Naive Bayes algorithm for classification\n",
    "# - Assumes features follow a normal distribution\n",
    "# - Simple and efficient for small to medium-sized datasets\n",
    "# - Works well with continuous features\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c8b641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the trained classifier to make predictions on the test data\n",
    "# - classifier.predict() returns predicted class labels for X_test\n",
    "# - Each prediction corresponds to a sample in X_test\n",
    "# - Returns numpy array of predicted class labels\n",
    "y_pred = classifier.predict(X_test) \n",
    "y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b30d9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2--Import confusion_matrix from scikit-learn's metrics module\n",
    "# confusion_matrix computes the confusion matrix to evaluate classification accuracy\n",
    "# - Shows true positives, false positives, true negatives, and false negatives\n",
    "# - Helps visualize model performance and identify misclassifications\n",
    "# - Returns a 2D array where rows represent actual classes and columns represent predicted classes\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6616abdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import accuracy_score from scikit-learn's metrics module\n",
    "# accuracy_score calculates the accuracy of the classifier's predictions\n",
    "# - Compares predicted labels (y_pred) with true labels (y_test)\n",
    "# - Returns a float between 0 and 1 representing the proportion of correct predictions\n",
    "# - 1.0 means perfect accuracy, 0.0 means all predictions were wrong\n",
    "from sklearn.metrics import accuracy_score \n",
    "print (\"Accuracy : \", accuracy_score(y_test, y_pred))\n",
    "Accuracy :  1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61e1d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame to compare actual vs predicted values\n",
    "# - Uses pandas DataFrame to organize the results\n",
    "# - 'Real Values' column contains the true labels from y_test\n",
    "# - 'Predicted Values' column contains the model's predictions from y_pred\n",
    "# - Makes it easy to visually inspect model performance\n",
    "# - Helps identify where predictions match or differ from actual values\n",
    "df = pd.DataFrame({'Real Values':y_test, 'Predicted Values':y_pred})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0730b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9333\n",
      "Error Rate: 0.0667\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        11\n",
      "  versicolor       0.80      1.00      0.89         8\n",
      "   virginica       1.00      0.82      0.90        11\n",
      "\n",
      "    accuracy                           0.93        30\n",
      "   macro avg       0.93      0.94      0.93        30\n",
      "weighted avg       0.95      0.93      0.93        30\n",
      "\n",
      "Confusion Matrix Breakdown:\n",
      "\n",
      "Class: setosa\n",
      "TP: 11, FP: 0, FN: 0, TN: 19\n",
      "\n",
      "Class: versicolor\n",
      "TP: 8, FP: 2, FN: 0, TN: 20\n",
      "\n",
      "Class: virginica\n",
      "TP: 9, FP: 0, FN: 2, TN: 19\n"
     ]
    }
   ],
   "source": [
    "# 2---Import necessary libraries for classification metrics\n",
    "# - classification_report provides detailed metrics for each class\n",
    "# - numpy is used for array operations and unique value extraction\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Compute confusion matrix\n",
    "# - Creates a 2D array showing true vs predicted class distributions\n",
    "# - Helps visualize model's performance across different classes\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Calculate accuracy and error rate\n",
    "# - accuracy_score computes proportion of correct predictions\n",
    "# - error_rate is complement of accuracy (1 - accuracy)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "error_rate = 1 - accuracy\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Error Rate: {error_rate:.4f}\")\n",
    "\n",
    "# Print classification report\n",
    "# - Provides precision, recall, F1-score for each class\n",
    "# - Includes macro and weighted averages\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Compute and display confusion matrix breakdown\n",
    "# - Extracts unique class labels from test and prediction data\n",
    "# - For each class, calculates:\n",
    "#   * True Positives (TP): Correctly predicted instances\n",
    "#   * False Positives (FP): Incorrectly predicted as this class\n",
    "#   * False Negatives (FN): This class incorrectly predicted as others\n",
    "#   * True Negatives (TN): Correctly predicted as not this class\n",
    "print(\"Confusion Matrix Breakdown:\")\n",
    "labels = np.unique(np.concatenate((y_test, y_pred)))\n",
    "for i, label in enumerate(labels):\n",
    "    TP = cm[i, i]\n",
    "    FP = cm[:, i].sum() - TP\n",
    "    FN = cm[i, :].sum() - TP\n",
    "    TN = cm.sum() - (TP + FP + FN)\n",
    "    \n",
    "    print(f\"\\nClass: {label}\")\n",
    "    print(f\"TP: {TP}, FP: {FP}, FN: {FN}, TN: {TN}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 13.707806,
   "end_time": "2022-03-28T06:30:19.758101",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-03-28T06:30:06.050295",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
